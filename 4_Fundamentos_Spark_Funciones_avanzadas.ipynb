{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RubenMcCarty/Big-Data-/blob/main/4_Fundamentos_Spark_Funciones_avanzadas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9fu6VfRdK4o"
      },
      "source": [
        "# Fundamentos de Apache Spark: Funciones avanzadas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxQcNXVMdK4q"
      },
      "source": [
        "En este notebook aprenderemos algunas funciones avanzadas para optimizar el rendimiento de Spark, para imputar valores faltantes o a crear funciones definidas por el usuario (UDF)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/ull\n",
        "!wget -q  https://archive.apache.org/dist/spark/spark-3.2.3/spark-3.2.3-bin-hadoop3.2.tgz"
      ],
      "metadata": {
        "id": "Y1UY6IRGde1s"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# descompremir la version de spark\n",
        "!tar xf spark-3.2.3-bin-hadoop3.2.tgz"
      ],
      "metadata": {
        "id": "5piH2DL7diqa"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.3-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "F92p4uFCdjs0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar la librería findspark\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "py7gJaiIdnPs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar pyspark\n",
        "!pip install -q pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoYc3JsndoRB",
        "outputId": "4e9abb90-0eed-4650-f73b-1b8e87e87747"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mP5-6sqqdK4q"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0aqAOfoddK4q"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.functions import broadcast\n",
        "from pyspark.sql.types import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeIj0ZscdK4r"
      },
      "source": [
        "### Crea la sesión de SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3unEGGZLdK4r"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnOU01D-dK4r"
      },
      "source": [
        "### Crear el DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "R3U6gPYydK4r"
      },
      "outputs": [],
      "source": [
        "emp = [(1, \"AAA\", \"dept1\", 1000),\n",
        "    (2, \"BBB\", \"dept1\", 1100),\n",
        "    (3, \"CCC\", \"dept1\", 3000),\n",
        "    (4, \"DDD\", \"dept1\", 1500),\n",
        "    (5, \"EEE\", \"dept2\", 8000),\n",
        "    (6, \"FFF\", \"dept2\", 7200),\n",
        "    (7, \"GGG\", \"dept3\", 7100),\n",
        "    (None, None, None, 7500),\n",
        "    (9, \"III\", None, 4500),\n",
        "    (10, None, \"dept5\", 2500)]\n",
        "\n",
        "dept = [(\"dept1\", \"Department - 1\"),\n",
        "        (\"dept2\", \"Department - 2\"),\n",
        "        (\"dept3\", \"Department - 3\"),\n",
        "        (\"dept4\", \"Department - 4\")\n",
        "       ]\n",
        "\n",
        "df = spark.createDataFrame(emp, [\"id\", \"name\", \"dept\", \"salary\"])\n",
        "deptdf = spark.createDataFrame(dept, [\"id\", \"name\"])\n",
        "\n",
        "# Create Temp Tables\n",
        "df.createOrReplaceTempView(\"empdf\")\n",
        "deptdf.createOrReplaceTempView(\"deptdf\")\n",
        "\n",
        "# Save as HIVE tables.\n",
        "df.write.saveAsTable(\"hive_empdf\", mode = \"overwrite\")\n",
        "deptdf.write.saveAsTable(\"hive_deptdf\", mode = \"overwrite\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzaB7qfadK4r"
      },
      "source": [
        "### BroadCast Join"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1JGEpLFdK4r"
      },
      "source": [
        "El tamaño de la tabla de difusión es de 10 MB. Sin embargo, podemos cambiar el umbral hasta 8GB según la documentación oficial de Spark 2.3.\n",
        "\n",
        "* Podemos verificar el tamaño de la tabla de transmisión de la siguiente manera:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "Ri4lrN_DdK4r",
        "outputId": "b0461e5c-f19a-430c-fb59-f0f45538c15f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-8e7e46ee92e0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.sql.autoBroadcastJoinThreshold\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1024\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Default size of broadcast table is {0} MB.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: '10485760b'"
          ]
        }
      ],
      "source": [
        "size = int(spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")) / (1024 * 1024)\n",
        "print(\"Default size of broadcast table is {0} MB.\".format(size))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQqXjEYIdK4s"
      },
      "source": [
        "* Podemos establecer el tamaño de la tabla de transmisión para que diga 50 MB de la siguiente manera:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "1cL_0QROdK4s"
      },
      "outputs": [],
      "source": [
        "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 50 * 1024 * 1024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "R_Nj8ua4dK4s",
        "outputId": "21785f65-5ff9-49eb-9ab6-6543c1c84eed"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-36763bc94cf6>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# big_df: DataFrame grande que debe unirse con DataFrame pequeño.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mjoin_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbig_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbroadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmall_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbig_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0msmall_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'big_df' is not defined"
          ]
        }
      ],
      "source": [
        "# Considere que necesitamos unir 2 Dataframes.\n",
        "# small_df: DataFrame pequeño que puede caber en la memoria y es más pequeño que el umbral especificado.\n",
        "# big_df: DataFrame grande que debe unirse con DataFrame pequeño.\n",
        "\n",
        "join_df = big_df.join(broadcast(small_df), big_df[\"id\"] == small_df[\"id\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ldGKOOodK4s"
      },
      "source": [
        "### Almacenamiento en caché\n",
        "Podemos usar la función de caché / persistencia para mantener el marco de datos en la memoria. Puede mejorar significativamente el rendimiento de su aplicación Spark si almacenamos en caché los datos que necesitamos usar con mucha frecuencia en nuestra aplicación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSrjLtVTdK4s",
        "outputId": "c90db1b8-0293-45be-ff3f-d76ac58193f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory Used : True\n",
            "Disk Used : True\n"
          ]
        }
      ],
      "source": [
        "df.cache()\n",
        "df.count()\n",
        "print(\"Memory Used : {0}\".format(df.storageLevel.useMemory))\n",
        "print(\"Disk Used : {0}\".format(df.storageLevel.useDisk))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAAN1RfUdK4s"
      },
      "source": [
        "Cuando usamos la función de caché, usará el nivel de almacenamiento como Memory_Only hasta Spark 2.0.2. Desde Spark 2.1.x es Memory_and_DISK.\n",
        "\n",
        "Sin embargo, si necesitamos especificar los distintos niveles de almacenamiento disponibles, podemos usar el método persist( ). Por ejemplo, si necesitamos mantener los datos solo en la memoria, podemos usar el siguiente fragmento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "5a_W4fDydK4s"
      },
      "outputs": [],
      "source": [
        "from pyspark.storagelevel import StorageLevel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gdl8LiovdK4t",
        "outputId": "84f84a4f-fdb9-4fb3-f3ea-5b0f09d1b59a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory Used : True\n",
            "Disk Used : True\n"
          ]
        }
      ],
      "source": [
        "deptdf.persist(StorageLevel.MEMORY_ONLY)\n",
        "deptdf.count()\n",
        "print(\"Memory Used : {0}\".format(df.storageLevel.useMemory))\n",
        "print(\"Disk Used : {0}\".format(df.storageLevel.useDisk))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-xjf4lBdK4t"
      },
      "source": [
        "### No persistir\n",
        "También es importante eliminar la memoria caché de los datos cuando ya no sean necesarios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZwSJ9tNdK4t",
        "outputId": "a5c751c3-0ba2-4b6a-e09c-2bc09f3c2709"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[id: bigint, name: string, dept: string, salary: bigint]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "df.unpersist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "J-0NVN99dK4t",
        "outputId": "299d3d51-fb1b-4448-81ee-441973d4e19a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-328e3a646197>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclearCache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'sqlContext' is not defined"
          ]
        }
      ],
      "source": [
        "sqlContext.clearCache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXmfvqQHdK4t"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scAW7mtjdK4t"
      },
      "source": [
        "#  Expresiones SQL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADs960vRdK4t"
      },
      "source": [
        "También podemos usar la expresión SQL para la manipulación de datos. Tenemos la función **expr** y también una variante de un método de selección como **selectExpr** para la evaluación de expresiones SQL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ra6kzQ7ddK4t",
        "outputId": "26f0dfcb-1bbb-4060-88bf-1389a40afc4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+-----+------+------------+\n",
            "|  id|name| dept|salary|salary_level|\n",
            "+----+----+-----+------+------------+\n",
            "|   1| AAA|dept1|  1000|  low_salary|\n",
            "|   2| BBB|dept1|  1100|  low_salary|\n",
            "|   3| CCC|dept1|  3000|  mid_salary|\n",
            "|   4| DDD|dept1|  1500|  low_salary|\n",
            "|   5| EEE|dept2|  8000| high_salary|\n",
            "|   6| FFF|dept2|  7200| high_salary|\n",
            "|   7| GGG|dept3|  7100| high_salary|\n",
            "|null|null| null|  7500| high_salary|\n",
            "|   9| III| null|  4500|  mid_salary|\n",
            "|  10|null|dept5|  2500|  mid_salary|\n",
            "+----+----+-----+------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import expr\n",
        "\n",
        "# Intentemos categorizar el salario en Bajo, Medio y Alto según la categorización a continuación.\n",
        "\n",
        "# 0-2000: salario_bajo\n",
        "# 2001 - 5000: mid_salary\n",
        "#> 5001: high_salary\n",
        "\n",
        "cond = \"\"\"case when salary > 5000 then 'high_salary'\n",
        "               else case when salary > 2000 then 'mid_salary'\n",
        "                    else case when salary > 0 then 'low_salary'\n",
        "                         else 'invalid_salary'\n",
        "                              end\n",
        "                         end\n",
        "                end as salary_level\"\"\"\n",
        "\n",
        "newdf = df.withColumn(\"salary_level\", expr(cond))\n",
        "newdf.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdlTJpiXdK4t"
      },
      "source": [
        "### Usando la función selectExpr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RP8FYFQMdK4t",
        "outputId": "f6a1824e-7a08-4b71-acab-ce68dabe06ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+----+-----+------+------------+\n",
            "|  id|name| dept|salary|salary_level|\n",
            "+----+----+-----+------+------------+\n",
            "|   1| AAA|dept1|  1000|  low_salary|\n",
            "|   2| BBB|dept1|  1100|  low_salary|\n",
            "|   3| CCC|dept1|  3000|  mid_salary|\n",
            "|   4| DDD|dept1|  1500|  low_salary|\n",
            "|   5| EEE|dept2|  8000| high_salary|\n",
            "|   6| FFF|dept2|  7200| high_salary|\n",
            "|   7| GGG|dept3|  7100| high_salary|\n",
            "|null|null| null|  7500| high_salary|\n",
            "|   9| III| null|  4500|  mid_salary|\n",
            "|  10|null|dept5|  2500|  mid_salary|\n",
            "+----+----+-----+------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "newdf = df.selectExpr(\"*\", cond)\n",
        "newdf.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wT7-i0jKdK4t"
      },
      "source": [
        "### Funciones definidas por el usuario (UDF)\n",
        "A menudo necesitamos escribir la función en función de nuestro requisito muy específico. Aquí podemos aprovechar las udfs. Podemos escribir nuestras propias funciones en un lenguaje como python y registrar la función como udf, luego podemos usar la función para operaciones de DataFrame.\n",
        "\n",
        "* Función de Python para encontrar el nivel_salario para un salario dado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "b-0h72u5dK4u"
      },
      "outputs": [],
      "source": [
        "def detSalary_Level(sal):\n",
        "    level = None\n",
        "\n",
        "    if(sal > 5000):\n",
        "        level = 'high_salary'\n",
        "    elif(sal > 2000):\n",
        "        level = 'mid_salary'\n",
        "    elif(sal > 0):\n",
        "        level = 'low_salary'\n",
        "    else:\n",
        "        level = 'invalid_salary'\n",
        "    return level"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqxVJ45_dK4u"
      },
      "source": [
        "* Luego registre la función \"detSalary_Level\" como UDF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "_sm9NsDPdK4u"
      },
      "outputs": [],
      "source": [
        "sal_level = udf(detSalary_Level, StringType())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9-gCovqdK4u"
      },
      "source": [
        "* Aplicar función para determinar el salario_level para un salario dado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuNmrDgUdK4u",
        "outputId": "ee134116-2e8b-4f1b-8ad8-3c349d077f9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+-----+------+------------+\n",
            "|  id|name| dept|salary|salary_level|\n",
            "+----+----+-----+------+------------+\n",
            "|   1| AAA|dept1|  1000|  low_salary|\n",
            "|   2| BBB|dept1|  1100|  low_salary|\n",
            "|   3| CCC|dept1|  3000|  mid_salary|\n",
            "|   4| DDD|dept1|  1500|  low_salary|\n",
            "|   5| EEE|dept2|  8000| high_salary|\n",
            "|   6| FFF|dept2|  7200| high_salary|\n",
            "|   7| GGG|dept3|  7100| high_salary|\n",
            "|null|null| null|  7500| high_salary|\n",
            "|   9| III| null|  4500|  mid_salary|\n",
            "|  10|null|dept5|  2500|  mid_salary|\n",
            "+----+----+-----+------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "newdf = df.withColumn(\"salary_level\", sal_level(\"salary\"))\n",
        "newdf.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0CYYi-BdK4u"
      },
      "source": [
        "### Trabajando con valores NULL\n",
        "\n",
        "Los valores NULL siempre son difíciles de manejar independientemente del Framework o lenguaje que usemos. Aquí en Spark tenemos pocas funciones específicas para lidiar con valores NULL.\n",
        "\n",
        "- **es nulo()**\n",
        "\n",
        "Esta función nos ayudará a encontrar los valores nulos para cualquier columna dada. Por ejemplo si necesitamos encontrar las columnas donde las columnas id contienen los valores nulos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajr68IeMdK4u",
        "outputId": "5cbdc2a5-6361-41db-a6b1-931ceaee86ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+----+------+\n",
            "|  id|name|dept|salary|\n",
            "+----+----+----+------+\n",
            "|null|null|null|  7500|\n",
            "|   9| III|null|  4500|\n",
            "+----+----+----+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "newdf = df.filter(df[\"dept\"].isNull())\n",
        "newdf.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VpyKQ8JdK4u"
      },
      "source": [
        "* **No es nulo()**\n",
        "\n",
        "Esta función funciona de manera opuesta a la función isNull () y devolverá todos los valores no nulos para una función en particular."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CX8e0aYFdK4u",
        "outputId": "54b52ef5-4598-47ce-c8de-1d94c16e6d0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+-----+------+\n",
            "| id|name| dept|salary|\n",
            "+---+----+-----+------+\n",
            "|  1| AAA|dept1|  1000|\n",
            "|  2| BBB|dept1|  1100|\n",
            "|  3| CCC|dept1|  3000|\n",
            "|  4| DDD|dept1|  1500|\n",
            "|  5| EEE|dept2|  8000|\n",
            "|  6| FFF|dept2|  7200|\n",
            "|  7| GGG|dept3|  7100|\n",
            "| 10|null|dept5|  2500|\n",
            "+---+----+-----+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "newdf = df.filter(df[\"dept\"].isNotNull())\n",
        "newdf.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_VhCG0ndK4u"
      },
      "source": [
        "* **fillna ()**\n",
        "\n",
        "Esta función nos ayudará a reemplazar los valores nulos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqhyC2-AdK4y",
        "outputId": "70f44ef9-7587-41ca-b1b4-9f420dc5111a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+-------+------+\n",
            "|  id|name|   dept|salary|\n",
            "+----+----+-------+------+\n",
            "|   1| AAA|  dept1|  1000|\n",
            "|   2| BBB|  dept1|  1100|\n",
            "|   3| CCC|  dept1|  3000|\n",
            "|   4| DDD|  dept1|  1500|\n",
            "|   5| EEE|  dept2|  8000|\n",
            "|   6| FFF|  dept2|  7200|\n",
            "|   7| GGG|  dept3|  7100|\n",
            "|null|null|INVALID|  7500|\n",
            "|   9| III|INVALID|  4500|\n",
            "|  10|null|  dept5|  2500|\n",
            "+----+----+-------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Replace -1 where the salary is null.\n",
        "newdf = df.fillna(\"INVALID\", [\"dept\"])\n",
        "newdf.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEIDctfHdK4y"
      },
      "source": [
        "* **dropna ()**\n",
        "\n",
        "Esta función nos ayudará a eliminar las filas con valores nulos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7jVBW1idK4y",
        "outputId": "8022d32a-70c4-46a3-a8c9-6673f2795187"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+-----+------+\n",
            "| id|name| dept|salary|\n",
            "+---+----+-----+------+\n",
            "|  1| AAA|dept1|  1000|\n",
            "|  2| BBB|dept1|  1100|\n",
            "|  3| CCC|dept1|  3000|\n",
            "|  4| DDD|dept1|  1500|\n",
            "|  5| EEE|dept2|  8000|\n",
            "|  6| FFF|dept2|  7200|\n",
            "|  7| GGG|dept3|  7100|\n",
            "+---+----+-----+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Remove all rows which contains any null values.\n",
        "newdf = df.dropna()\n",
        "newdf.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgvmY1lZdK4y",
        "outputId": "7226633e-7944-4051-9bbf-5ac54f7d3222"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+-----+------+\n",
            "|  id|name| dept|salary|\n",
            "+----+----+-----+------+\n",
            "|   1| AAA|dept1|  1000|\n",
            "|   2| BBB|dept1|  1100|\n",
            "|   3| CCC|dept1|  3000|\n",
            "|   4| DDD|dept1|  1500|\n",
            "|   5| EEE|dept2|  8000|\n",
            "|   6| FFF|dept2|  7200|\n",
            "|   7| GGG|dept3|  7100|\n",
            "|null|null| null|  7500|\n",
            "|   9| III| null|  4500|\n",
            "|  10|null|dept5|  2500|\n",
            "+----+----+-----+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Elimina todas las filas que contienen todos los valores nulos.\n",
        "newdf = df.dropna(how = \"all\")\n",
        "newdf.show()\n",
        "\n",
        "# Nota: valor predeterminado de \"cómo\" param es \"any\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQvDuJhrdK4y",
        "outputId": "25fcac4f-391a-4da0-ce62-502888eaf378"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+-----+------+\n",
            "| id|name| dept|salary|\n",
            "+---+----+-----+------+\n",
            "|  1| AAA|dept1|  1000|\n",
            "|  2| BBB|dept1|  1100|\n",
            "|  3| CCC|dept1|  3000|\n",
            "|  4| DDD|dept1|  1500|\n",
            "|  5| EEE|dept2|  8000|\n",
            "|  6| FFF|dept2|  7200|\n",
            "|  7| GGG|dept3|  7100|\n",
            "| 10|null|dept5|  2500|\n",
            "+---+----+-----+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Remove all rows where columns : dept is null.\n",
        "newdf = df.dropna(subset = \"dept\")\n",
        "newdf.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZJ34gRqdK4y"
      },
      "source": [
        "## Partitioning\n",
        "\n",
        "\n",
        "El particionamiento es un aspecto muy importante para controlar el paralelismo de la aplicación Spark."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjMbZr46dK4y"
      },
      "source": [
        "* Consultar número de particiones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keH6_Op-dK4z",
        "outputId": "b441b89b-6b85-419c-bd5d-8188396e005a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "df.rdd.getNumPartitions()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JNlQIpidK4z"
      },
      "source": [
        "* Incrementar el número de particiones. Por ejemplo Aumentar las particiones a 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIyc-IDSdK4z",
        "outputId": "08f3faf2-e9e8-46ea-e6f7-b30b74ba6706"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "newdf = df.repartition(6)\n",
        "newdf.rdd.getNumPartitions()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4r-v8ZYgdK4z"
      },
      "source": [
        "**Nota: se trata de operaciones costosas, ya que requiere la mezcla de datos entre los trabajadores.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DjVDnpTdK4z"
      },
      "source": [
        "* Disminuir el número de particiones. Por ejemplo disminuir las particiones a 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qnfpp9WdK4z",
        "outputId": "f5766422-adaf-4e10-a5df-e0ff0ebd03e2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "newdf = df.coalesce(2)\n",
        "newdf.rdd.getNumPartitions()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldunjSvxdK4z"
      },
      "source": [
        "* De forma predeterminada, el número de particiones para Spark SQL es 200.\n",
        "* Pero también podemos establecer el número de particiones en el nivel de aplicación Spark. Por ejemplo establecido en 500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czk0NJTRdK4z",
        "outputId": "45610378-50b3-44e1-d1f5-c32e69225483"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No of Partitions : 500\n"
          ]
        }
      ],
      "source": [
        "# Set number of partitions as Spark Application.\n",
        "spark.conf.set(\"spark.sql.shuffle.partitions\", \"500\")\n",
        "\n",
        "# Check the number of patitions.\n",
        "num_part = spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
        "print(\"No of Partitions : {0}\".format(num_part))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMJn4J84dK4z"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buPjygeGdK4z"
      },
      "source": [
        "# Catálogo de APIs\n",
        "\n",
        "Spark Catalog es una API orientada al usuario, a la que puede acceder mediante SparkSession.catalog."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nBVYqdhdK4z"
      },
      "source": [
        "* **listDatabases ()**\n",
        "\n",
        "Devolverá todas las bases de datos junto con su ubicación en el sistema de archivos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fhl5JwdKdK40",
        "outputId": "cc9e7272-1a51-4bb5-a3a5-c6699ffe0491"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Database(name='default', description='default database', locationUri='file:/content/spark-warehouse')]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "spark.catalog.listDatabases()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNdg_PJidK40"
      },
      "source": [
        "* **listTables ()**\n",
        "\n",
        "Devolverá todas las tablas para una base de datos determinada junto con información como el tipo de tabla (externa / administrada) y si una tabla en particular es temporal o permanente.\n",
        "Esto incluye todas las vistas temporales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqEcRFiudK40",
        "outputId": "6d856946-f09f-4d51-ad7b-194420e545ac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Table(name='hive_deptdf', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
              " Table(name='hive_empdf', database='default', description=None, tableType='MANAGED', isTemporary=False),\n",
              " Table(name='deptdf', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
              " Table(name='empdf', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "spark.catalog.listTables(\"default\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbHAMm3odK40"
      },
      "source": [
        "* **listColumns ()**\n",
        "\n",
        "Devolverá todas las columnas de una tabla en particular en DataBase. Además, devolverá el tipo de datos, si la columna se usa en particiones o agrupaciones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8O685HBDdK40",
        "outputId": "8446a89a-3f2d-4058-8afb-71f0ad759377"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Column(name='id', description=None, dataType='bigint', nullable=True, isPartition=False, isBucket=False),\n",
              " Column(name='name', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
              " Column(name='dept', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
              " Column(name='salary', description=None, dataType='bigint', nullable=True, isPartition=False, isBucket=False)]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "spark.catalog.listColumns(\"hive_empdf\", \"default\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twFAsew_dK40"
      },
      "source": [
        "* **listFunctions()**\n",
        "\n",
        "Devolverá todas las funciones disponibles en Spark Session junto con la información si es temporal o no."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjEIzrdpdK40",
        "outputId": "0cf5539c-08af-4b30-acd4-9d0d671643fe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Function(name='!', description=None, className='org.apache.spark.sql.catalyst.expressions.Not', isTemporary=True),\n",
              " Function(name='%', description=None, className='org.apache.spark.sql.catalyst.expressions.Remainder', isTemporary=True),\n",
              " Function(name='&', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseAnd', isTemporary=True),\n",
              " Function(name='*', description=None, className='org.apache.spark.sql.catalyst.expressions.Multiply', isTemporary=True),\n",
              " Function(name='+', description=None, className='org.apache.spark.sql.catalyst.expressions.Add', isTemporary=True),\n",
              " Function(name='-', description=None, className='org.apache.spark.sql.catalyst.expressions.Subtract', isTemporary=True),\n",
              " Function(name='/', description=None, className='org.apache.spark.sql.catalyst.expressions.Divide', isTemporary=True),\n",
              " Function(name='<', description=None, className='org.apache.spark.sql.catalyst.expressions.LessThan', isTemporary=True),\n",
              " Function(name='<=', description=None, className='org.apache.spark.sql.catalyst.expressions.LessThanOrEqual', isTemporary=True),\n",
              " Function(name='<=>', description=None, className='org.apache.spark.sql.catalyst.expressions.EqualNullSafe', isTemporary=True),\n",
              " Function(name='=', description=None, className='org.apache.spark.sql.catalyst.expressions.EqualTo', isTemporary=True),\n",
              " Function(name='==', description=None, className='org.apache.spark.sql.catalyst.expressions.EqualTo', isTemporary=True),\n",
              " Function(name='>', description=None, className='org.apache.spark.sql.catalyst.expressions.GreaterThan', isTemporary=True),\n",
              " Function(name='>=', description=None, className='org.apache.spark.sql.catalyst.expressions.GreaterThanOrEqual', isTemporary=True),\n",
              " Function(name='^', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseXor', isTemporary=True),\n",
              " Function(name='abs', description=None, className='org.apache.spark.sql.catalyst.expressions.Abs', isTemporary=True),\n",
              " Function(name='acos', description=None, className='org.apache.spark.sql.catalyst.expressions.Acos', isTemporary=True),\n",
              " Function(name='acosh', description=None, className='org.apache.spark.sql.catalyst.expressions.Acosh', isTemporary=True),\n",
              " Function(name='add_months', description=None, className='org.apache.spark.sql.catalyst.expressions.AddMonths', isTemporary=True),\n",
              " Function(name='aggregate', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayAggregate', isTemporary=True),\n",
              " Function(name='and', description=None, className='org.apache.spark.sql.catalyst.expressions.And', isTemporary=True),\n",
              " Function(name='any', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BoolOr', isTemporary=True),\n",
              " Function(name='approx_count_distinct', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.HyperLogLogPlusPlus', isTemporary=True),\n",
              " Function(name='approx_percentile', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.ApproximatePercentile', isTemporary=True),\n",
              " Function(name='array', description=None, className='org.apache.spark.sql.catalyst.expressions.CreateArray', isTemporary=True),\n",
              " Function(name='array_contains', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayContains', isTemporary=True),\n",
              " Function(name='array_distinct', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayDistinct', isTemporary=True),\n",
              " Function(name='array_except', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayExcept', isTemporary=True),\n",
              " Function(name='array_intersect', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayIntersect', isTemporary=True),\n",
              " Function(name='array_join', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayJoin', isTemporary=True),\n",
              " Function(name='array_max', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayMax', isTemporary=True),\n",
              " Function(name='array_min', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayMin', isTemporary=True),\n",
              " Function(name='array_position', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayPosition', isTemporary=True),\n",
              " Function(name='array_remove', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayRemove', isTemporary=True),\n",
              " Function(name='array_repeat', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayRepeat', isTemporary=True),\n",
              " Function(name='array_sort', description=None, className='org.apache.spark.sql.catalyst.expressions.ArraySort', isTemporary=True),\n",
              " Function(name='array_union', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayUnion', isTemporary=True),\n",
              " Function(name='arrays_overlap', description=None, className='org.apache.spark.sql.catalyst.expressions.ArraysOverlap', isTemporary=True),\n",
              " Function(name='arrays_zip', description=None, className='org.apache.spark.sql.catalyst.expressions.ArraysZip', isTemporary=True),\n",
              " Function(name='ascii', description=None, className='org.apache.spark.sql.catalyst.expressions.Ascii', isTemporary=True),\n",
              " Function(name='asin', description=None, className='org.apache.spark.sql.catalyst.expressions.Asin', isTemporary=True),\n",
              " Function(name='asinh', description=None, className='org.apache.spark.sql.catalyst.expressions.Asinh', isTemporary=True),\n",
              " Function(name='assert_true', description=None, className='org.apache.spark.sql.catalyst.expressions.AssertTrue', isTemporary=True),\n",
              " Function(name='atan', description=None, className='org.apache.spark.sql.catalyst.expressions.Atan', isTemporary=True),\n",
              " Function(name='atan2', description=None, className='org.apache.spark.sql.catalyst.expressions.Atan2', isTemporary=True),\n",
              " Function(name='atanh', description=None, className='org.apache.spark.sql.catalyst.expressions.Atanh', isTemporary=True),\n",
              " Function(name='avg', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Average', isTemporary=True),\n",
              " Function(name='base64', description=None, className='org.apache.spark.sql.catalyst.expressions.Base64', isTemporary=True),\n",
              " Function(name='bigint', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
              " Function(name='bin', description=None, className='org.apache.spark.sql.catalyst.expressions.Bin', isTemporary=True),\n",
              " Function(name='binary', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
              " Function(name='bit_and', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BitAndAgg', isTemporary=True),\n",
              " Function(name='bit_count', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseCount', isTemporary=True),\n",
              " Function(name='bit_get', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseGet', isTemporary=True),\n",
              " Function(name='bit_length', description=None, className='org.apache.spark.sql.catalyst.expressions.BitLength', isTemporary=True),\n",
              " Function(name='bit_or', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BitOrAgg', isTemporary=True),\n",
              " Function(name='bit_xor', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BitXorAgg', isTemporary=True),\n",
              " Function(name='bool_and', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BoolAnd', isTemporary=True),\n",
              " Function(name='bool_or', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BoolOr', isTemporary=True),\n",
              " Function(name='boolean', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
              " Function(name='bround', description=None, className='org.apache.spark.sql.catalyst.expressions.BRound', isTemporary=True),\n",
              " Function(name='btrim', description=None, className='org.apache.spark.sql.catalyst.expressions.StringTrimBoth', isTemporary=True),\n",
              " Function(name='cardinality', description=None, className='org.apache.spark.sql.catalyst.expressions.Size', isTemporary=True),\n",
              " Function(name='cast', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
              " Function(name='cbrt', description=None, className='org.apache.spark.sql.catalyst.expressions.Cbrt', isTemporary=True),\n",
              " Function(name='ceil', description=None, className='org.apache.spark.sql.catalyst.expressions.Ceil', isTemporary=True),\n",
              " Function(name='ceiling', description=None, className='org.apache.spark.sql.catalyst.expressions.Ceil', isTemporary=True),\n",
              " Function(name='char', description=None, className='org.apache.spark.sql.catalyst.expressions.Chr', isTemporary=True),\n",
              " Function(name='char_length', description=None, className='org.apache.spark.sql.catalyst.expressions.Length', isTemporary=True),\n",
              " Function(name='character_length', description=None, className='org.apache.spark.sql.catalyst.expressions.Length', isTemporary=True),\n",
              " Function(name='chr', description=None, className='org.apache.spark.sql.catalyst.expressions.Chr', isTemporary=True),\n",
              " Function(name='coalesce', description=None, className='org.apache.spark.sql.catalyst.expressions.Coalesce', isTemporary=True),\n",
              " Function(name='collect_list', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CollectList', isTemporary=True),\n",
              " Function(name='collect_set', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CollectSet', isTemporary=True),\n",
              " Function(name='concat', description=None, className='org.apache.spark.sql.catalyst.expressions.Concat', isTemporary=True),\n",
              " Function(name='concat_ws', description=None, className='org.apache.spark.sql.catalyst.expressions.ConcatWs', isTemporary=True),\n",
              " Function(name='conv', description=None, className='org.apache.spark.sql.catalyst.expressions.Conv', isTemporary=True),\n",
              " Function(name='corr', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Corr', isTemporary=True),\n",
              " Function(name='cos', description=None, className='org.apache.spark.sql.catalyst.expressions.Cos', isTemporary=True),\n",
              " Function(name='cosh', description=None, className='org.apache.spark.sql.catalyst.expressions.Cosh', isTemporary=True),\n",
              " Function(name='cot', description=None, className='org.apache.spark.sql.catalyst.expressions.Cot', isTemporary=True),\n",
              " Function(name='count', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Count', isTemporary=True),\n",
              " Function(name='count_if', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CountIf', isTemporary=True),\n",
              " Function(name='count_min_sketch', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CountMinSketchAgg', isTemporary=True),\n",
              " Function(name='covar_pop', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CovPopulation', isTemporary=True),\n",
              " Function(name='covar_samp', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CovSample', isTemporary=True),\n",
              " Function(name='crc32', description=None, className='org.apache.spark.sql.catalyst.expressions.Crc32', isTemporary=True),\n",
              " Function(name='cume_dist', description=None, className='org.apache.spark.sql.catalyst.expressions.CumeDist', isTemporary=True),\n",
              " Function(name='current_catalog', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentCatalog', isTemporary=True),\n",
              " Function(name='current_database', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentDatabase', isTemporary=True),\n",
              " Function(name='current_date', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentDate', isTemporary=True),\n",
              " Function(name='current_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentTimestamp', isTemporary=True),\n",
              " Function(name='current_timezone', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentTimeZone', isTemporary=True),\n",
              " Function(name='current_user', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentUser', isTemporary=True),\n",
              " Function(name='date', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
              " Function(name='date_add', description=None, className='org.apache.spark.sql.catalyst.expressions.DateAdd', isTemporary=True),\n",
              " Function(name='date_format', description=None, className='org.apache.spark.sql.catalyst.expressions.DateFormatClass', isTemporary=True),\n",
              " Function(name='date_from_unix_date', description=None, className='org.apache.spark.sql.catalyst.expressions.DateFromUnixDate', isTemporary=True),\n",
              " Function(name='date_part', description=None, className='org.apache.spark.sql.catalyst.expressions.DatePart', isTemporary=True),\n",
              " Function(name='date_sub', description=None, className='org.apache.spark.sql.catalyst.expressions.DateSub', isTemporary=True),\n",
              " Function(name='date_trunc', description=None, className='org.apache.spark.sql.catalyst.expressions.TruncTimestamp', isTemporary=True),\n",
              " Function(name='datediff', description=None, className='org.apache.spark.sql.catalyst.expressions.DateDiff', isTemporary=True),\n",
              " Function(name='day', description=None, className='org.apache.spark.sql.catalyst.expressions.DayOfMonth', isTemporary=True),\n",
              " Function(name='dayofmonth', description=None, className='org.apache.spark.sql.catalyst.expressions.DayOfMonth', isTemporary=True),\n",
              " Function(name='dayofweek', description=None, className='org.apache.spark.sql.catalyst.expressions.DayOfWeek', isTemporary=True),\n",
              " Function(name='dayofyear', description=None, className='org.apache.spark.sql.catalyst.expressions.DayOfYear', isTemporary=True),\n",
              " Function(name='decimal', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
              " Function(name='decode', description=None, className='org.apache.spark.sql.catalyst.expressions.Decode', isTemporary=True),\n",
              " Function(name='degrees', description=None, className='org.apache.spark.sql.catalyst.expressions.ToDegrees', isTemporary=True),\n",
              " Function(name='dense_rank', description=None, className='org.apache.spark.sql.catalyst.expressions.DenseRank', isTemporary=True),\n",
              " Function(name='div', description=None, className='org.apache.spark.sql.catalyst.expressions.IntegralDivide', isTemporary=True),\n",
              " Function(name='double', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
              " Function(name='e', description=None, className='org.apache.spark.sql.catalyst.expressions.EulerNumber', isTemporary=True),\n",
              " Function(name='element_at', description=None, className='org.apache.spark.sql.catalyst.expressions.ElementAt', isTemporary=True),\n",
              " Function(name='elt', description=None, className='org.apache.spark.sql.catalyst.expressions.Elt', isTemporary=True),\n",
              " Function(name='encode', description=None, className='org.apache.spark.sql.catalyst.expressions.Encode', isTemporary=True),\n",
              " Function(name='every', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BoolAnd', isTemporary=True),\n",
              " Function(name='exists', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayExists', isTemporary=True),\n",
              " Function(name='exp', description=None, className='org.apache.spark.sql.catalyst.expressions.Exp', isTemporary=True),\n",
              " Function(name='explode', description=None, className='org.apache.spark.sql.catalyst.expressions.Explode', isTemporary=True),\n",
              " Function(name='explode_outer', description=None, className='org.apache.spark.sql.catalyst.expressions.Explode', isTemporary=True),\n",
              " Function(name='expm1', description=None, className='org.apache.spark.sql.catalyst.expressions.Expm1', isTemporary=True),\n",
              " Function(name='extract', description=None, className='org.apache.spark.sql.catalyst.expressions.Extract', isTemporary=True),\n",
              " Function(name='factorial', description=None, className='org.apache.spark.sql.catalyst.expressions.Factorial', isTemporary=True),\n",
              " Function(name='filter', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayFilter', isTemporary=True),\n",
              " Function(name='find_in_set', description=None, className='org.apache.spark.sql.catalyst.expressions.FindInSet', isTemporary=True),\n",
              " Function(name='first', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.First', isTemporary=True),\n",
              " Function(name='first_value', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.First', isTemporary=True),\n",
              " Function(name='flatten', description=None, className='org.apache.spark.sql.catalyst.expressions.Flatten', isTemporary=True),\n",
              " Function(name='float', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
              " Function(name='floor', description=None, className='org.apache.spark.sql.catalyst.expressions.Floor', isTemporary=True),\n",
              " Function(name='forall', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayForAll', isTemporary=True),\n",
              " Function(name='format_number', description=None, className='org.apache.spark.sql.catalyst.expressions.FormatNumber', isTemporary=True),\n",
              " Function(name='format_string', description=None, className='org.apache.spark.sql.catalyst.expressions.FormatString', isTemporary=True),\n",
              " Function(name='from_csv', description=None, className='org.apache.spark.sql.catalyst.expressions.CsvToStructs', isTemporary=True),\n",
              " Function(name='from_json', description=None, className='org.apache.spark.sql.catalyst.expressions.JsonToStructs', isTemporary=True),\n",
              " Function(name='from_unixtime', description=None, className='org.apache.spark.sql.catalyst.expressions.FromUnixTime', isTemporary=True),\n",
              " Function(name='from_utc_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.FromUTCTimestamp', isTemporary=True),\n",
              " Function(name='get_json_object', description=None, className='org.apache.spark.sql.catalyst.expressions.GetJsonObject', isTemporary=True),\n",
              " Function(name='getbit', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseGet', isTemporary=True),\n",
              " Function(name='greatest', description=None, className='org.apache.spark.sql.catalyst.expressions.Greatest', isTemporary=True),\n",
              " Function(name='grouping', description=None, className='org.apache.spark.sql.catalyst.expressions.Grouping', isTemporary=True),\n",
              " Function(name='grouping_id', description=None, className='org.apache.spark.sql.catalyst.expressions.GroupingID', isTemporary=True),\n",
              " Function(name='hash', description=None, className='org.apache.spark.sql.catalyst.expressions.Murmur3Hash', isTemporary=True),\n",
              " Function(name='hex', description=None, className='org.apache.spark.sql.catalyst.expressions.Hex', isTemporary=True),\n",
              " Function(name='hour', description=None, className='org.apache.spark.sql.catalyst.expressions.Hour', isTemporary=True),\n",
              " Function(name='hypot', description=None, className='org.apache.spark.sql.catalyst.expressions.Hypot', isTemporary=True),\n",
              " Function(name='if', description=None, className='org.apache.spark.sql.catalyst.expressions.If', isTemporary=True),\n",
              " Function(name='ifnull', description=None, className='org.apache.spark.sql.catalyst.expressions.IfNull', isTemporary=True),\n",
              " Function(name='in', description=None, className='org.apache.spark.sql.catalyst.expressions.In', isTemporary=True),\n",
              " Function(name='initcap', description=None, className='org.apache.spark.sql.catalyst.expressions.InitCap', isTemporary=True),\n",
              " Function(name='inline', description=None, className='org.apache.spark.sql.catalyst.expressions.Inline', isTemporary=True),\n",
              " Function(name='inline_outer', description=None, className='org.apache.spark.sql.catalyst.expressions.Inline', isTemporary=True),\n",
              " Function(name='input_file_block_length', description=None, className='org.apache.spark.sql.catalyst.expressions.InputFileBlockLength', isTemporary=True),\n",
              " Function(name='input_file_block_start', description=None, className='org.apache.spark.sql.catalyst.expressions.InputFileBlockStart', isTemporary=True),\n",
              " Function(name='input_file_name', description=None, className='org.apache.spark.sql.catalyst.expressions.InputFileName', isTemporary=True),\n",
              " Function(name='instr', description=None, className='org.apache.spark.sql.catalyst.expressions.StringInstr', isTemporary=True),\n",
              " Function(name='int', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
              " Function(name='isnan', description=None, className='org.apache.spark.sql.catalyst.expressions.IsNaN', isTemporary=True),\n",
              " Function(name='isnotnull', description=None, className='org.apache.spark.sql.catalyst.expressions.IsNotNull', isTemporary=True),\n",
              " Function(name='isnull', description=None, className='org.apache.spark.sql.catalyst.expressions.IsNull', isTemporary=True),\n",
              " Function(name='java_method', description=None, className='org.apache.spark.sql.catalyst.expressions.CallMethodViaReflection', isTemporary=True),\n",
              " Function(name='json_array_length', description=None, className='org.apache.spark.sql.catalyst.expressions.LengthOfJsonArray', isTemporary=True),\n",
              " Function(name='json_object_keys', description=None, className='org.apache.spark.sql.catalyst.expressions.JsonObjectKeys', isTemporary=True),\n",
              " Function(name='json_tuple', description=None, className='org.apache.spark.sql.catalyst.expressions.JsonTuple', isTemporary=True),\n",
              " Function(name='kurtosis', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Kurtosis', isTemporary=True),\n",
              " Function(name='lag', description=None, className='org.apache.spark.sql.catalyst.expressions.Lag', isTemporary=True),\n",
              " Function(name='last', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Last', isTemporary=True),\n",
              " Function(name='last_day', description=None, className='org.apache.spark.sql.catalyst.expressions.LastDay', isTemporary=True),\n",
              " Function(name='last_value', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Last', isTemporary=True),\n",
              " Function(name='lcase', description=None, className='org.apache.spark.sql.catalyst.expressions.Lower', isTemporary=True),\n",
              " Function(name='lead', description=None, className='org.apache.spark.sql.catalyst.expressions.Lead', isTemporary=True),\n",
              " Function(name='least', description=None, className='org.apache.spark.sql.catalyst.expressions.Least', isTemporary=True),\n",
              " Function(name='left', description=None, className='org.apache.spark.sql.catalyst.expressions.Left', isTemporary=True),\n",
              " Function(name='length', description=None, className='org.apache.spark.sql.catalyst.expressions.Length', isTemporary=True),\n",
              " Function(name='levenshtein', description=None, className='org.apache.spark.sql.catalyst.expressions.Levenshtein', isTemporary=True),\n",
              " Function(name='like', description=None, className='org.apache.spark.sql.catalyst.expressions.Like', isTemporary=True),\n",
              " Function(name='ln', description=None, className='org.apache.spark.sql.catalyst.expressions.Log', isTemporary=True),\n",
              " Function(name='locate', description=None, className='org.apache.spark.sql.catalyst.expressions.StringLocate', isTemporary=True),\n",
              " Function(name='log', description=None, className='org.apache.spark.sql.catalyst.expressions.Logarithm', isTemporary=True),\n",
              " Function(name='log10', description=None, className='org.apache.spark.sql.catalyst.expressions.Log10', isTemporary=True),\n",
              " Function(name='log1p', description=None, className='org.apache.spark.sql.catalyst.expressions.Log1p', isTemporary=True),\n",
              " Function(name='log2', description=None, className='org.apache.spark.sql.catalyst.expressions.Log2', isTemporary=True),\n",
              " Function(name='lower', description=None, className='org.apache.spark.sql.catalyst.expressions.Lower', isTemporary=True),\n",
              " Function(name='lpad', description=None, className='org.apache.spark.sql.catalyst.expressions.StringLPad', isTemporary=True),\n",
              " Function(name='ltrim', description=None, className='org.apache.spark.sql.catalyst.expressions.StringTrimLeft', isTemporary=True),\n",
              " Function(name='make_date', description=None, className='org.apache.spark.sql.catalyst.expressions.MakeDate', isTemporary=True),\n",
              " Function(name='make_dt_interval', description=None, className='org.apache.spark.sql.catalyst.expressions.MakeDTInterval', isTemporary=True),\n",
              " Function(name='make_interval', description=None, className='org.apache.spark.sql.catalyst.expressions.MakeInterval', isTemporary=True),\n",
              " Function(name='make_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.MakeTimestamp', isTemporary=True),\n",
              " Function(name='make_ym_interval', description=None, className='org.apache.spark.sql.catalyst.expressions.MakeYMInterval', isTemporary=True),\n",
              " Function(name='map', description=None, className='org.apache.spark.sql.catalyst.expressions.CreateMap', isTemporary=True),\n",
              " Function(name='map_concat', description=None, className='org.apache.spark.sql.catalyst.expressions.MapConcat', isTemporary=True),\n",
              " Function(name='map_entries', description=None, className='org.apache.spark.sql.catalyst.expressions.MapEntries', isTemporary=True),\n",
              " Function(name='map_filter', description=None, className='org.apache.spark.sql.catalyst.expressions.MapFilter', isTemporary=True),\n",
              " Function(name='map_from_arrays', description=None, className='org.apache.spark.sql.catalyst.expressions.MapFromArrays', isTemporary=True),\n",
              " Function(name='map_from_entries', description=None, className='org.apache.spark.sql.catalyst.expressions.MapFromEntries', isTemporary=True),\n",
              " Function(name='map_keys', description=None, className='org.apache.spark.sql.catalyst.expressions.MapKeys', isTemporary=True),\n",
              " Function(name='map_values', description=None, className='org.apache.spark.sql.catalyst.expressions.MapValues', isTemporary=True),\n",
              " Function(name='map_zip_with', description=None, className='org.apache.spark.sql.catalyst.expressions.MapZipWith', isTemporary=True),\n",
              " Function(name='max', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Max', isTemporary=True),\n",
              " Function(name='max_by', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.MaxBy', isTemporary=True),\n",
              " Function(name='md5', description=None, className='org.apache.spark.sql.catalyst.expressions.Md5', isTemporary=True),\n",
              " Function(name='mean', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Average', isTemporary=True),\n",
              " Function(name='min', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Min', isTemporary=True),\n",
              " Function(name='min_by', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.MinBy', isTemporary=True),\n",
              " Function(name='minute', description=None, className='org.apache.spark.sql.catalyst.expressions.Minute', isTemporary=True),\n",
              " Function(name='mod', description=None, className='org.apache.spark.sql.catalyst.expressions.Remainder', isTemporary=True),\n",
              " Function(name='monotonically_increasing_id', description=None, className='org.apache.spark.sql.catalyst.expressions.MonotonicallyIncreasingID', isTemporary=True),\n",
              " Function(name='month', description=None, className='org.apache.spark.sql.catalyst.expressions.Month', isTemporary=True),\n",
              " Function(name='months_between', description=None, className='org.apache.spark.sql.catalyst.expressions.MonthsBetween', isTemporary=True),\n",
              " Function(name='named_struct', description=None, className='org.apache.spark.sql.catalyst.expressions.CreateNamedStruct', isTemporary=True),\n",
              " Function(name='nanvl', description=None, className='org.apache.spark.sql.catalyst.expressions.NaNvl', isTemporary=True),\n",
              " Function(name='negative', description=None, className='org.apache.spark.sql.catalyst.expressions.UnaryMinus', isTemporary=True),\n",
              " Function(name='next_day', description=None, className='org.apache.spark.sql.catalyst.expressions.NextDay', isTemporary=True),\n",
              " Function(name='not', description=None, className='org.apache.spark.sql.catalyst.expressions.Not', isTemporary=True),\n",
              " Function(name='now', description=None, className='org.apache.spark.sql.catalyst.expressions.Now', isTemporary=True),\n",
              " Function(name='nth_value', description=None, className='org.apache.spark.sql.catalyst.expressions.NthValue', isTemporary=True),\n",
              " Function(name='ntile', description=None, className='org.apache.spark.sql.catalyst.expressions.NTile', isTemporary=True),\n",
              " Function(name='nullif', description=None, className='org.apache.spark.sql.catalyst.expressions.NullIf', isTemporary=True),\n",
              " Function(name='nvl', description=None, className='org.apache.spark.sql.catalyst.expressions.Nvl', isTemporary=True),\n",
              " Function(name='nvl2', description=None, className='org.apache.spark.sql.catalyst.expressions.Nvl2', isTemporary=True),\n",
              " Function(name='octet_length', description=None, className='org.apache.spark.sql.catalyst.expressions.OctetLength', isTemporary=True),\n",
              " Function(name='or', description=None, className='org.apache.spark.sql.catalyst.expressions.Or', isTemporary=True),\n",
              " Function(name='overlay', description=None, className='org.apache.spark.sql.catalyst.expressions.Overlay', isTemporary=True),\n",
              " Function(name='parse_url', description=None, className='org.apache.spark.sql.catalyst.expressions.ParseUrl', isTemporary=True),\n",
              " Function(name='percent_rank', description=None, className='org.apache.spark.sql.catalyst.expressions.PercentRank', isTemporary=True),\n",
              " Function(name='percentile', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Percentile', isTemporary=True),\n",
              " Function(name='percentile_approx', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.ApproximatePercentile', isTemporary=True),\n",
              " Function(name='pi', description=None, className='org.apache.spark.sql.catalyst.expressions.Pi', isTemporary=True),\n",
              " Function(name='pmod', description=None, className='org.apache.spark.sql.catalyst.expressions.Pmod', isTemporary=True),\n",
              " Function(name='posexplode', description=None, className='org.apache.spark.sql.catalyst.expressions.PosExplode', isTemporary=True),\n",
              " Function(name='posexplode_outer', description=None, className='org.apache.spark.sql.catalyst.expressions.PosExplode', isTemporary=True),\n",
              " Function(name='position', description=None, className='org.apache.spark.sql.catalyst.expressions.StringLocate', isTemporary=True),\n",
              " Function(name='positive', description=None, className='org.apache.spark.sql.catalyst.expressions.UnaryPositive', isTemporary=True),\n",
              " Function(name='pow', description=None, className='org.apache.spark.sql.catalyst.expressions.Pow', isTemporary=True),\n",
              " Function(name='power', description=None, className='org.apache.spark.sql.catalyst.expressions.Pow', isTemporary=True),\n",
              " Function(name='printf', description=None, className='org.apache.spark.sql.catalyst.expressions.FormatString', isTemporary=True),\n",
              " Function(name='quarter', description=None, className='org.apache.spark.sql.catalyst.expressions.Quarter', isTemporary=True),\n",
              " Function(name='radians', description=None, className='org.apache.spark.sql.catalyst.expressions.ToRadians', isTemporary=True),\n",
              " Function(name='raise_error', description=None, className='org.apache.spark.sql.catalyst.expressions.RaiseError', isTemporary=True),\n",
              " Function(name='rand', description=None, className='org.apache.spark.sql.catalyst.expressions.Rand', isTemporary=True),\n",
              " Function(name='randn', description=None, className='org.apache.spark.sql.catalyst.expressions.Randn', isTemporary=True),\n",
              " Function(name='random', description=None, className='org.apache.spark.sql.catalyst.expressions.Rand', isTemporary=True),\n",
              " Function(name='range', description=None, className='org.apache.spark.sql.catalyst.plans.logical.Range', isTemporary=True),\n",
              " Function(name='rank', description=None, className='org.apache.spark.sql.catalyst.expressions.Rank', isTemporary=True),\n",
              " Function(name='reflect', description=None, className='org.apache.spark.sql.catalyst.expressions.CallMethodViaReflection', isTemporary=True),\n",
              " Function(name='regexp', description=None, className='org.apache.spark.sql.catalyst.expressions.RLike', isTemporary=True),\n",
              " Function(name='regexp_extract', description=None, className='org.apache.spark.sql.catalyst.expressions.RegExpExtract', isTemporary=True),\n",
              " Function(name='regexp_extract_all', description=None, className='org.apache.spark.sql.catalyst.expressions.RegExpExtractAll', isTemporary=True),\n",
              " Function(name='regexp_like', description=None, className='org.apache.spark.sql.catalyst.expressions.RLike', isTemporary=True),\n",
              " Function(name='regexp_replace', description=None, className='org.apache.spark.sql.catalyst.expressions.RegExpReplace', isTemporary=True),\n",
              " Function(name='repeat', description=None, className='org.apache.spark.sql.catalyst.expressions.StringRepeat', isTemporary=True),\n",
              " Function(name='replace', description=None, className='org.apache.spark.sql.catalyst.expressions.StringReplace', isTemporary=True),\n",
              " Function(name='reverse', description=None, className='org.apache.spark.sql.catalyst.expressions.Reverse', isTemporary=True),\n",
              " Function(name='right', description=None, className='org.apache.spark.sql.catalyst.expressions.Right', isTemporary=True),\n",
              " Function(name='rint', description=None, className='org.apache.spark.sql.catalyst.expressions.Rint', isTemporary=True),\n",
              " Function(name='rlike', description=None, className='org.apache.spark.sql.catalyst.expressions.RLike', isTemporary=True),\n",
              " Function(name='round', description=None, className='org.apache.spark.sql.catalyst.expressions.Round', isTemporary=True),\n",
              " Function(name='row_number', description=None, className='org.apache.spark.sql.catalyst.expressions.RowNumber', isTemporary=True),\n",
              " Function(name='rpad', description=None, className='org.apache.spark.sql.catalyst.expressions.StringRPad', isTemporary=True),\n",
              " Function(name='rtrim', description=None, className='org.apache.spark.sql.catalyst.expressions.StringTrimRight', isTemporary=True),\n",
              " Function(name='schema_of_csv', description=None, className='org.apache.spark.sql.catalyst.expressions.SchemaOfCsv', isTemporary=True),\n",
              " Function(name='schema_of_json', description=None, className='org.apache.spark.sql.catalyst.expressions.SchemaOfJson', isTemporary=True),\n",
              " Function(name='second', description=None, className='org.apache.spark.sql.catalyst.expressions.Second', isTemporary=True),\n",
              " Function(name='sentences', description=None, className='org.apache.spark.sql.catalyst.expressions.Sentences', isTemporary=True),\n",
              " Function(name='sequence', description=None, className='org.apache.spark.sql.catalyst.expressions.Sequence', isTemporary=True),\n",
              " Function(name='session_window', description=None, className='org.apache.spark.sql.catalyst.expressions.SessionWindow', isTemporary=True),\n",
              " Function(name='sha', description=None, className='org.apache.spark.sql.catalyst.expressions.Sha1', isTemporary=True),\n",
              " Function(name='sha1', description=None, className='org.apache.spark.sql.catalyst.expressions.Sha1', isTemporary=True),\n",
              " Function(name='sha2', description=None, className='org.apache.spark.sql.catalyst.expressions.Sha2', isTemporary=True),\n",
              " Function(name='shiftleft', description=None, className='org.apache.spark.sql.catalyst.expressions.ShiftLeft', isTemporary=True),\n",
              " Function(name='shiftright', description=None, className='org.apache.spark.sql.catalyst.expressions.ShiftRight', isTemporary=True),\n",
              " Function(name='shiftrightunsigned', description=None, className='org.apache.spark.sql.catalyst.expressions.ShiftRightUnsigned', isTemporary=True),\n",
              " Function(name='shuffle', description=None, className='org.apache.spark.sql.catalyst.expressions.Shuffle', isTemporary=True),\n",
              " Function(name='sign', description=None, className='org.apache.spark.sql.catalyst.expressions.Signum', isTemporary=True),\n",
              " Function(name='signum', description=None, className='org.apache.spark.sql.catalyst.expressions.Signum', isTemporary=True),\n",
              " Function(name='sin', description=None, className='org.apache.spark.sql.catalyst.expressions.Sin', isTemporary=True),\n",
              " Function(name='sinh', description=None, className='org.apache.spark.sql.catalyst.expressions.Sinh', isTemporary=True),\n",
              " Function(name='size', description=None, className='org.apache.spark.sql.catalyst.expressions.Size', isTemporary=True),\n",
              " Function(name='skewness', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Skewness', isTemporary=True),\n",
              " Function(name='slice', description=None, className='org.apache.spark.sql.catalyst.expressions.Slice', isTemporary=True),\n",
              " Function(name='smallint', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
              " Function(name='some', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BoolOr', isTemporary=True),\n",
              " Function(name='sort_array', description=None, className='org.apache.spark.sql.catalyst.expressions.SortArray', isTemporary=True),\n",
              " Function(name='soundex', description=None, className='org.apache.spark.sql.catalyst.expressions.SoundEx', isTemporary=True),\n",
              " Function(name='space', description=None, className='org.apache.spark.sql.catalyst.expressions.StringSpace', isTemporary=True),\n",
              " Function(name='spark_partition_id', description=None, className='org.apache.spark.sql.catalyst.expressions.SparkPartitionID', isTemporary=True),\n",
              " Function(name='split', description=None, className='org.apache.spark.sql.catalyst.expressions.StringSplit', isTemporary=True),\n",
              " Function(name='sqrt', description=None, className='org.apache.spark.sql.catalyst.expressions.Sqrt', isTemporary=True),\n",
              " Function(name='stack', description=None, className='org.apache.spark.sql.catalyst.expressions.Stack', isTemporary=True),\n",
              " Function(name='std', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.StddevSamp', isTemporary=True),\n",
              " Function(name='stddev', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.StddevSamp', isTemporary=True),\n",
              " Function(name='stddev_pop', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.StddevPop', isTemporary=True),\n",
              " Function(name='stddev_samp', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.StddevSamp', isTemporary=True),\n",
              " Function(name='str_to_map', description=None, className='org.apache.spark.sql.catalyst.expressions.StringToMap', isTemporary=True),\n",
              " Function(name='string', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
              " Function(name='struct', description=None, className='org.apache.spark.sql.catalyst.expressions.CreateNamedStruct', isTemporary=True),\n",
              " Function(name='substr', description=None, className='org.apache.spark.sql.catalyst.expressions.Substring', isTemporary=True),\n",
              " Function(name='substring', description=None, className='org.apache.spark.sql.catalyst.expressions.Substring', isTemporary=True),\n",
              " Function(name='substring_index', description=None, className='org.apache.spark.sql.catalyst.expressions.SubstringIndex', isTemporary=True),\n",
              " Function(name='sum', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Sum', isTemporary=True),\n",
              " Function(name='tan', description=None, className='org.apache.spark.sql.catalyst.expressions.Tan', isTemporary=True),\n",
              " Function(name='tanh', description=None, className='org.apache.spark.sql.catalyst.expressions.Tanh', isTemporary=True),\n",
              " Function(name='timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
              " Function(name='timestamp_micros', description=None, className='org.apache.spark.sql.catalyst.expressions.MicrosToTimestamp', isTemporary=True),\n",
              " Function(name='timestamp_millis', description=None, className='org.apache.spark.sql.catalyst.expressions.MillisToTimestamp', isTemporary=True),\n",
              " Function(name='timestamp_seconds', description=None, className='org.apache.spark.sql.catalyst.expressions.SecondsToTimestamp', isTemporary=True),\n",
              " Function(name='tinyint', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
              " Function(name='to_csv', description=None, className='org.apache.spark.sql.catalyst.expressions.StructsToCsv', isTemporary=True),\n",
              " Function(name='to_date', description=None, className='org.apache.spark.sql.catalyst.expressions.ParseToDate', isTemporary=True),\n",
              " Function(name='to_json', description=None, className='org.apache.spark.sql.catalyst.expressions.StructsToJson', isTemporary=True),\n",
              " Function(name='to_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.ParseToTimestamp', isTemporary=True),\n",
              " Function(name='to_unix_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.ToUnixTimestamp', isTemporary=True),\n",
              " Function(name='to_utc_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.ToUTCTimestamp', isTemporary=True),\n",
              " Function(name='transform', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayTransform', isTemporary=True),\n",
              " Function(name='transform_keys', description=None, className='org.apache.spark.sql.catalyst.expressions.TransformKeys', isTemporary=True),\n",
              " Function(name='transform_values', description=None, className='org.apache.spark.sql.catalyst.expressions.TransformValues', isTemporary=True),\n",
              " Function(name='translate', description=None, className='org.apache.spark.sql.catalyst.expressions.StringTranslate', isTemporary=True),\n",
              " Function(name='trim', description=None, className='org.apache.spark.sql.catalyst.expressions.StringTrim', isTemporary=True),\n",
              " Function(name='trunc', description=None, className='org.apache.spark.sql.catalyst.expressions.TruncDate', isTemporary=True),\n",
              " Function(name='try_add', description=None, className='org.apache.spark.sql.catalyst.expressions.TryAdd', isTemporary=True),\n",
              " Function(name='try_divide', description=None, className='org.apache.spark.sql.catalyst.expressions.TryDivide', isTemporary=True),\n",
              " Function(name='typeof', description=None, className='org.apache.spark.sql.catalyst.expressions.TypeOf', isTemporary=True),\n",
              " Function(name='ucase', description=None, className='org.apache.spark.sql.catalyst.expressions.Upper', isTemporary=True),\n",
              " Function(name='unbase64', description=None, className='org.apache.spark.sql.catalyst.expressions.UnBase64', isTemporary=True),\n",
              " Function(name='unhex', description=None, className='org.apache.spark.sql.catalyst.expressions.Unhex', isTemporary=True),\n",
              " Function(name='unix_date', description=None, className='org.apache.spark.sql.catalyst.expressions.UnixDate', isTemporary=True),\n",
              " Function(name='unix_micros', description=None, className='org.apache.spark.sql.catalyst.expressions.UnixMicros', isTemporary=True),\n",
              " Function(name='unix_millis', description=None, className='org.apache.spark.sql.catalyst.expressions.UnixMillis', isTemporary=True),\n",
              " Function(name='unix_seconds', description=None, className='org.apache.spark.sql.catalyst.expressions.UnixSeconds', isTemporary=True),\n",
              " Function(name='unix_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.UnixTimestamp', isTemporary=True),\n",
              " Function(name='upper', description=None, className='org.apache.spark.sql.catalyst.expressions.Upper', isTemporary=True),\n",
              " Function(name='uuid', description=None, className='org.apache.spark.sql.catalyst.expressions.Uuid', isTemporary=True),\n",
              " Function(name='var_pop', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.VariancePop', isTemporary=True),\n",
              " Function(name='var_samp', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.VarianceSamp', isTemporary=True),\n",
              " Function(name='variance', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.VarianceSamp', isTemporary=True),\n",
              " Function(name='version', description=None, className='org.apache.spark.sql.catalyst.expressions.SparkVersion', isTemporary=True),\n",
              " Function(name='weekday', description=None, className='org.apache.spark.sql.catalyst.expressions.WeekDay', isTemporary=True),\n",
              " Function(name='weekofyear', description=None, className='org.apache.spark.sql.catalyst.expressions.WeekOfYear', isTemporary=True),\n",
              " Function(name='when', description=None, className='org.apache.spark.sql.catalyst.expressions.CaseWhen', isTemporary=True),\n",
              " Function(name='width_bucket', description=None, className='org.apache.spark.sql.catalyst.expressions.WidthBucket', isTemporary=True),\n",
              " Function(name='window', description=None, className='org.apache.spark.sql.catalyst.expressions.TimeWindow', isTemporary=True),\n",
              " Function(name='xpath', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathList', isTemporary=True),\n",
              " Function(name='xpath_boolean', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathBoolean', isTemporary=True),\n",
              " Function(name='xpath_double', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathDouble', isTemporary=True),\n",
              " Function(name='xpath_float', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathFloat', isTemporary=True),\n",
              " Function(name='xpath_int', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathInt', isTemporary=True),\n",
              " Function(name='xpath_long', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathLong', isTemporary=True),\n",
              " Function(name='xpath_number', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathDouble', isTemporary=True),\n",
              " Function(name='xpath_short', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathShort', isTemporary=True),\n",
              " Function(name='xpath_string', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathString', isTemporary=True),\n",
              " Function(name='xxhash64', description=None, className='org.apache.spark.sql.catalyst.expressions.XxHash64', isTemporary=True),\n",
              " Function(name='year', description=None, className='org.apache.spark.sql.catalyst.expressions.Year', isTemporary=True),\n",
              " Function(name='zip_with', description=None, className='org.apache.spark.sql.catalyst.expressions.ZipWith', isTemporary=True),\n",
              " Function(name='|', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseOr', isTemporary=True),\n",
              " Function(name='~', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseNot', isTemporary=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "spark.catalog.listFunctions()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2BXT5pQdK40"
      },
      "source": [
        "* **currentDatabase ()**\n",
        "\n",
        "Obtenga la base de datos actual."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "XGIegnEWdK40",
        "outputId": "7a2d4f81-9fef-4b1c-af57-7001c0e465c7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'default'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "spark.catalog.currentDatabase()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjmf54o4dK40"
      },
      "source": [
        "* **setCurrentDatabase ()**\n",
        "\n",
        "Establecer la base de datos actual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "b16nIerbdK40",
        "outputId": "a3a6b36a-c2ec-4f1a-f6fa-9c5486022c86"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-47-dd4d5f75237a>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    spark.catalog.setCurrentDatabase(<DB_Name>)\u001b[0m\n\u001b[0m                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "\n",
        "spark.catalog.setCurrentDatabase(<DB_Name>)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AIDFJsrdK41"
      },
      "source": [
        "* **cacheTable ()**\n",
        "\n",
        "almacenar en caché una tabla en particular.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "xV3QXmaodK41"
      },
      "outputs": [],
      "source": [
        "spark.catalog.cacheTable(\"default.hive_empdf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-loQHiSdK41"
      },
      "source": [
        "* **isCached()**\n",
        "\n",
        "Compruebe si la tabla está almacenada en caché o no."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iz00iDSVdK41",
        "outputId": "268b65b3-f3a7-461b-a3d9-ffc76c268b92"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "spark.catalog.isCached(\"default.hive_empdf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUyW0zSadK41"
      },
      "source": [
        "* **uncacheTable()**\n",
        "\n",
        "Des-cachear de una tabla en particular."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "55uuNeofdK41"
      },
      "outputs": [],
      "source": [
        "spark.catalog.uncacheTable(\"default.hive_empdf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAWEm61zdK41",
        "outputId": "5371e0d2-123e-4928-eb4e-8ff57172c90f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "# Verify uncached table. Now you will see that it will return \"False\" which means table is not cached.\n",
        "spark.catalog.isCached(\"default.hive_empdf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pb7ulbAZdK41"
      },
      "source": [
        "* **clearCache()**\n",
        "\n",
        "Des-cachear toda la tabla en la sesión de Spark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "lls3PqzbdK41"
      },
      "outputs": [],
      "source": [
        "spark.catalog.clearCache()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}