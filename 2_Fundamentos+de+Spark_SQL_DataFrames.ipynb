{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RubenMcCarty/Big-Data-/blob/main/2_Fundamentos%2Bde%2BSpark_SQL_DataFrames.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfGDdFFeMDKX"
      },
      "source": [
        "# Fundamentos de Apache Spark: SQL/DataFrames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hleiNGC-MDKY"
      },
      "source": [
        "**Spark SQLtrabaja con DataFrames**. Un DataFrame es una **representación relacional de los datos**. Proporciona funciones con capacidades similares a SQL. Además, permite escribir **consultas tipo SQL** para nuestro análisis de datos.\n",
        "\n",
        "Los DataFrames son similares a las tablas relacionales o DataFrames en Python / R auqnue con muchas optimizaciones que se ejecutan de manera \"oculta\" para el usuario. Hay varias formas de crear DataFrames a partir de colecciones, tablas HIVE, tablas relacionales y RDD."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/ull\n",
        "!wget -q  https://archive.apache.org/dist/spark/spark-3.2.3/spark-3.2.3-bin-hadoop3.2.tgz"
      ],
      "metadata": {
        "id": "nle83xXbMT9S"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# descompremir la version de spark\n",
        "!tar xf spark-3.2.3-bin-hadoop3.2.tgz"
      ],
      "metadata": {
        "id": "WylUqC_VMXFs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.3-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "QLFunJJLMZ8-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar la librería findspark\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "6Altjv92M_fH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar pyspark\n",
        "!pip install -q pyspark"
      ],
      "metadata": {
        "id": "L7X4iZMWNAwD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "6RmsvfNeMDKZ"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "import pandas as pd\n",
        "import pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "5cJ70Z3mMDKa"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6wRjgbiMDKa"
      },
      "source": [
        "### Crear la sesión de Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "BTSWiFd2MDKa"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kupu7JbbMDKa"
      },
      "source": [
        "### Crear el DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "RMQf7wEMMDKa"
      },
      "outputs": [],
      "source": [
        "emp = [(1, \"AAA\", \"dept1\", 1000),\n",
        "    (2, \"BBB\", \"dept1\", 1100),\n",
        "    (3, \"CCC\", \"dept1\", 3000),\n",
        "    (4, \"DDD\", \"dept1\", 1500),\n",
        "    (5, \"EEE\", \"dept2\", 8000),\n",
        "    (6, \"FFF\", \"dept2\", 7200),\n",
        "    (7, \"GGG\", \"dept3\", 7100),\n",
        "    (8, \"HHH\", \"dept3\", 3700),\n",
        "    (9, \"III\", \"dept3\", 4500),\n",
        "    (10, \"JJJ\", \"dept5\", 3400)]\n",
        "\n",
        "dept = [(\"dept1\", \"Department - 1\"),\n",
        "        (\"dept2\", \"Department - 2\"),\n",
        "        (\"dept3\", \"Department - 3\"),\n",
        "        (\"dept4\", \"Department - 4\")\n",
        "\n",
        "       ]\n",
        "\n",
        "df = spark.createDataFrame(emp, [\"id\", \"name\", \"dept\", \"salary\"])\n",
        "\n",
        "deptdf = spark.createDataFrame(dept, [\"id\", \"name\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_dDTdWFMDKa",
        "outputId": "35848da9-cb31-45a8-a6e3-ff79d76ba638"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+-----+------+\n",
            "| id|name| dept|salary|\n",
            "+---+----+-----+------+\n",
            "|  1| AAA|dept1|  1000|\n",
            "|  2| BBB|dept1|  1100|\n",
            "|  3| CCC|dept1|  3000|\n",
            "|  4| DDD|dept1|  1500|\n",
            "|  5| EEE|dept2|  8000|\n",
            "|  6| FFF|dept2|  7200|\n",
            "|  7| GGG|dept3|  7100|\n",
            "|  8| HHH|dept3|  3700|\n",
            "|  9| III|dept3|  4500|\n",
            "| 10| JJJ|dept5|  3400|\n",
            "+---+----+-----+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "pBN1FtLrMDKb",
        "outputId": "c8772ff1-8466-4c77-af50-417b5c5927e6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-630c22aadc1c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Crear un df a partir de una tabla de Hive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tbl_name\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#df = spark.table(\"tbl_name\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.3-bin-hadoop3.2/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mtable\u001b[0;34m(self, tableName)\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m         \"\"\"\n\u001b[0;32m--> 741\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtableName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.3-bin-hadoop3.2/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.3-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: Table or view not found: tbl_name;\n'UnresolvedRelation [tbl_name], [], false\n"
          ]
        }
      ],
      "source": [
        "#Crear un df a partir de una tabla de Hive\n",
        "df = spark.table(\"tbl_name\")\n",
        "#df = spark.table(\"tbl_name\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qALgNeqMDKb"
      },
      "source": [
        "# Operaciones básicas en DataFrames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8IcwcBmMDKb"
      },
      "source": [
        "### count\n",
        "* Cuenta el número de filas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vy0lx9aMDKb",
        "outputId": "8308a2bc-71a7-49eb-9c55-0c121eff9c7f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "df.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdxiLWggMDKc"
      },
      "source": [
        "### columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWMOioXuMDKc",
        "outputId": "e428678c-8575-495c-dd66-fc6c090b34e6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['id', 'name', 'dept', 'salary']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDw3CtRsMDKc"
      },
      "source": [
        "### dtypes\n",
        "** Accede al DataType de columnas dentro del DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QO7ipnqMDKc",
        "outputId": "25eeee49-1572-4e56-e86a-ecab5e82b4f0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('id', 'bigint'),\n",
              " ('name', 'string'),\n",
              " ('dept', 'string'),\n",
              " ('salary', 'bigint')]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0LMsSUqMDKc"
      },
      "source": [
        "### schema\n",
        "** Comprueba cómo Spark almacena el esquema del DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBY7zx94MDKc",
        "outputId": "5f129379-21ec-4d16-d6ad-cdf8689db8aa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StructType(List(StructField(id,LongType,true),StructField(name,StringType,true),StructField(dept,StringType,true),StructField(salary,LongType,true)))"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "df.schema"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwDz_i6iMDKc"
      },
      "source": [
        "### printSchema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZPKkxB-MDKc",
        "outputId": "da8812fe-b259-4f78-e306-dd5a54c487bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- dept: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1jVp-KKMDKc"
      },
      "source": [
        "### select\n",
        "* Seleccione columnas del DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BmHqiKBMDKc",
        "outputId": "421e03f3-3423-4b8f-b66a-40632899952d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+\n",
            "| id|name|\n",
            "+---+----+\n",
            "|  1| AAA|\n",
            "|  2| BBB|\n",
            "|  3| CCC|\n",
            "|  4| DDD|\n",
            "|  5| EEE|\n",
            "|  6| FFF|\n",
            "|  7| GGG|\n",
            "|  8| HHH|\n",
            "|  9| III|\n",
            "| 10| JJJ|\n",
            "+---+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.select(\"id\", \"name\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKq5-nazMDKd"
      },
      "source": [
        "### filter\n",
        "\n",
        "* Filtrar las filas según alguna condición.\n",
        "* Intentemos encontrar las filas con id = 1.\n",
        "* Hay diferentes formas de especificar la condición."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmNdPPOYMDKd",
        "outputId": "0761daf2-56a2-4efb-8cd4-2c0c86972813"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+-----+------+\n",
            "| id|name| dept|salary|\n",
            "+---+----+-----+------+\n",
            "|  1| AAA|dept1|  1000|\n",
            "+---+----+-----+------+\n",
            "\n",
            "+---+----+-----+------+\n",
            "| id|name| dept|salary|\n",
            "+---+----+-----+------+\n",
            "|  1| AAA|dept1|  1000|\n",
            "+---+----+-----+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.filter(df[\"id\"] == 1).show()\n",
        "df.filter(df.id == 1).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9uN9GTsMDKd",
        "outputId": "d8263bde-b3f3-41a6-ad1f-fee25e6dfb25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+-----+------+\n",
            "| id|name| dept|salary|\n",
            "+---+----+-----+------+\n",
            "|  1| AAA|dept1|  1000|\n",
            "+---+----+-----+------+\n",
            "\n",
            "+---+----+-----+------+\n",
            "| id|name| dept|salary|\n",
            "+---+----+-----+------+\n",
            "|  1| AAA|dept1|  1000|\n",
            "+---+----+-----+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.filter(col(\"id\") == 1).show()\n",
        "df.filter(\"id = 1\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aD_cvMKHMDKd"
      },
      "source": [
        "### drop\n",
        "* Elimina una columna en particular"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXL7xyoYMDKd",
        "outputId": "1d230357-7e6c-41d2-d7f7-ec7ab8abadd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+------+\n",
            "|name| dept|salary|\n",
            "+----+-----+------+\n",
            "| AAA|dept1|  1000|\n",
            "| BBB|dept1|  1100|\n",
            "+----+-----+------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "newdf = df.drop(\"id\")\n",
        "newdf.show(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsRgl7V_MDKd"
      },
      "source": [
        "### Aggregations\n",
        "* Podemos usar la función groupBy para agrupar los datos y luego usar la función \"agg\" para realizar la agregación de datos agrupados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsdAxQDmMDKd",
        "outputId": "4c449acd-348c-4ed0-8126-288edcfe2398"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----+----+----+------+\n",
            "| dept|count|  sum| max| min|   avg|\n",
            "+-----+-----+-----+----+----+------+\n",
            "|dept1|    4| 6600|3000|1000|1650.0|\n",
            "|dept2|    2|15200|8000|7200|7600.0|\n",
            "|dept5|    1| 3400|3400|3400|3400.0|\n",
            "|dept3|    3|15300|7100|3700|5100.0|\n",
            "+-----+-----+-----+----+----+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "(df.groupBy(\"dept\")\n",
        "    .agg(\n",
        "        count(\"salary\").alias(\"count\"),\n",
        "        sum(\"salary\").alias(\"sum\"),\n",
        "        max(\"salary\").alias(\"max\"),\n",
        "        min(\"salary\").alias(\"min\"),\n",
        "        avg(\"salary\").alias(\"avg\")\n",
        "        ).show()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtI6fR8mMDKd"
      },
      "source": [
        "### Sorting\n",
        "\n",
        "* Ordena los datos según el \"salario\". De forma predeterminada, la clasificación se realizará en orden ascendente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5VGbZP4MDKd",
        "outputId": "39ff6aad-d953-486c-da08-052bf6cb1436"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+-----+------+\n",
            "| id|name| dept|salary|\n",
            "+---+----+-----+------+\n",
            "|  1| AAA|dept1|  1000|\n",
            "|  2| BBB|dept1|  1100|\n",
            "|  4| DDD|dept1|  1500|\n",
            "|  3| CCC|dept1|  3000|\n",
            "| 10| JJJ|dept5|  3400|\n",
            "+---+----+-----+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.sort(\"salary\").show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BB_fZxWLMDKe",
        "outputId": "9e14d1a3-1302-4ca2-8b33-f525529f6daf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+-----+------+\n",
            "| id|name| dept|salary|\n",
            "+---+----+-----+------+\n",
            "|  5| EEE|dept2|  8000|\n",
            "|  6| FFF|dept2|  7200|\n",
            "|  7| GGG|dept3|  7100|\n",
            "|  9| III|dept3|  4500|\n",
            "|  8| HHH|dept3|  3700|\n",
            "+---+----+-----+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sort the data in descending order.\n",
        "df.sort(desc(\"salary\")).show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHnLTI2EMDKe"
      },
      "source": [
        "### Columnas derivadas\n",
        "* Podemos usar la función \"withColumn\" para derivar la columna en función de las columnas existentes ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1s4__TmhMDKe",
        "outputId": "f52c338d-8329-4ebe-de5a-89b6b378a753"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+-----+------+-----+\n",
            "| id|name| dept|salary|bonus|\n",
            "+---+----+-----+------+-----+\n",
            "|  1| AAA|dept1|  1000|100.0|\n",
            "|  2| BBB|dept1|  1100|110.0|\n",
            "|  3| CCC|dept1|  3000|300.0|\n",
            "|  4| DDD|dept1|  1500|150.0|\n",
            "|  5| EEE|dept2|  8000|800.0|\n",
            "|  6| FFF|dept2|  7200|720.0|\n",
            "|  7| GGG|dept3|  7100|710.0|\n",
            "|  8| HHH|dept3|  3700|370.0|\n",
            "|  9| III|dept3|  4500|450.0|\n",
            "| 10| JJJ|dept5|  3400|340.0|\n",
            "+---+----+-----+------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.withColumn(\"bonus\", col(\"salary\") * .1).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0TITLqNMDKe"
      },
      "source": [
        "### Joins\n",
        "\n",
        "* Podemos realizar varios tipos de combinaciones en múltiples DataFrames."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dda19Jg0MDKe"
      },
      "source": [
        "![Sin%20t%C3%ADtulo.png](attachment:Sin%20t%C3%ADtulo.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EdsPy9nMDKe",
        "outputId": "bd73ea75-e4e6-45d5-f835-b92ca4d77f90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+-----+------+-----+--------------+\n",
            "| id|name| dept|salary|   id|          name|\n",
            "+---+----+-----+------+-----+--------------+\n",
            "|  1| AAA|dept1|  1000|dept1|Department - 1|\n",
            "|  2| BBB|dept1|  1100|dept1|Department - 1|\n",
            "|  3| CCC|dept1|  3000|dept1|Department - 1|\n",
            "|  4| DDD|dept1|  1500|dept1|Department - 1|\n",
            "|  5| EEE|dept2|  8000|dept2|Department - 2|\n",
            "|  6| FFF|dept2|  7200|dept2|Department - 2|\n",
            "|  7| GGG|dept3|  7100|dept3|Department - 3|\n",
            "|  8| HHH|dept3|  3700|dept3|Department - 3|\n",
            "|  9| III|dept3|  4500|dept3|Department - 3|\n",
            "+---+----+-----+------+-----+--------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Inner JOIN.\n",
        "df.join(deptdf, df[\"dept\"] == deptdf[\"id\"]).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8BFUHUNMDKe"
      },
      "source": [
        "### Left Outer Join"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HN502CNbMDKi",
        "outputId": "4385e540-0755-4df3-94c6-e30d5c140863"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+-----+------+-----+--------------+\n",
            "| id|name| dept|salary|   id|          name|\n",
            "+---+----+-----+------+-----+--------------+\n",
            "|  1| AAA|dept1|  1000|dept1|Department - 1|\n",
            "|  2| BBB|dept1|  1100|dept1|Department - 1|\n",
            "|  3| CCC|dept1|  3000|dept1|Department - 1|\n",
            "|  4| DDD|dept1|  1500|dept1|Department - 1|\n",
            "|  5| EEE|dept2|  8000|dept2|Department - 2|\n",
            "|  6| FFF|dept2|  7200|dept2|Department - 2|\n",
            "|  7| GGG|dept3|  7100|dept3|Department - 3|\n",
            "|  8| HHH|dept3|  3700|dept3|Department - 3|\n",
            "|  9| III|dept3|  4500|dept3|Department - 3|\n",
            "| 10| JJJ|dept5|  3400| null|          null|\n",
            "+---+----+-----+------+-----+--------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.join(deptdf, df[\"dept\"] == deptdf[\"id\"], \"left_outer\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "src7RQE2MDKi"
      },
      "source": [
        "### Right Outer Join"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utsEPI1jMDKi",
        "outputId": "95cc3bce-d433-4c56-e400-14270ac335f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+-----+------+-----+--------------+\n",
            "|  id|name| dept|salary|   id|          name|\n",
            "+----+----+-----+------+-----+--------------+\n",
            "|   1| AAA|dept1|  1000|dept1|Department - 1|\n",
            "|   2| BBB|dept1|  1100|dept1|Department - 1|\n",
            "|   3| CCC|dept1|  3000|dept1|Department - 1|\n",
            "|   4| DDD|dept1|  1500|dept1|Department - 1|\n",
            "|   5| EEE|dept2|  8000|dept2|Department - 2|\n",
            "|   6| FFF|dept2|  7200|dept2|Department - 2|\n",
            "|   7| GGG|dept3|  7100|dept3|Department - 3|\n",
            "|   8| HHH|dept3|  3700|dept3|Department - 3|\n",
            "|   9| III|dept3|  4500|dept3|Department - 3|\n",
            "|null|null| null|  null|dept4|Department - 4|\n",
            "+----+----+-----+------+-----+--------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.join(deptdf, df[\"dept\"] == deptdf[\"id\"], \"right_outer\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4X6h2KnMDKi"
      },
      "source": [
        "### Full Outer Join"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oShfoWKzMDKi",
        "outputId": "224b68f6-736d-44d6-c0e1-25f761267b3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+-----+------+-----+--------------+\n",
            "|  id|name| dept|salary|   id|          name|\n",
            "+----+----+-----+------+-----+--------------+\n",
            "|   1| AAA|dept1|  1000|dept1|Department - 1|\n",
            "|   2| BBB|dept1|  1100|dept1|Department - 1|\n",
            "|   3| CCC|dept1|  3000|dept1|Department - 1|\n",
            "|   4| DDD|dept1|  1500|dept1|Department - 1|\n",
            "|   5| EEE|dept2|  8000|dept2|Department - 2|\n",
            "|   6| FFF|dept2|  7200|dept2|Department - 2|\n",
            "|   7| GGG|dept3|  7100|dept3|Department - 3|\n",
            "|   8| HHH|dept3|  3700|dept3|Department - 3|\n",
            "|   9| III|dept3|  4500|dept3|Department - 3|\n",
            "|null|null| null|  null|dept4|Department - 4|\n",
            "|  10| JJJ|dept5|  3400| null|          null|\n",
            "+----+----+-----+------+-----+--------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.join(deptdf, df[\"dept\"] == deptdf[\"id\"], \"outer\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdCJVvPsMDKi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pB8zb0aVMDKi"
      },
      "source": [
        "### Consultas SQL\n",
        "* Ejecución de consultas tipo SQL.\n",
        "* También podemos realizar análisis de datos escribiendo consultas similares a SQL. Para realizar consultas similares a SQL, necesitamos registrar el DataFrame como una Vista temporal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXR49fR9MDKi",
        "outputId": "ef5a6cef-30dd-4df5-c541-8ddb7b344fc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+-----+------+\n",
            "| id|name| dept|salary|\n",
            "+---+----+-----+------+\n",
            "|  1| AAA|dept1|  1000|\n",
            "+---+----+-----+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Register DataFrame as Temporary Table\n",
        "df.createOrReplaceTempView(\"temp_table\")\n",
        "\n",
        "# Execute SQL-Like query.\n",
        "spark.sql(\"select * from temp_table where id = 1\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbqXxtWQMDKi",
        "outputId": "4130a927-f705-420b-e168-5e937593afa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  5|\n",
            "|  1|\n",
            "|  3|\n",
            "|  2|\n",
            "|  4|\n",
            "|  7|\n",
            "|  6|\n",
            "|  9|\n",
            "| 10|\n",
            "|  8|\n",
            "+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"select distinct id from temp_table\").show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkkTPIwTMDKj",
        "outputId": "152ba4eb-0bd1-4dac-a7d0-c9fb2fe971b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+-----+------+\n",
            "| id|name| dept|salary|\n",
            "+---+----+-----+------+\n",
            "|  3| CCC|dept1|  3000|\n",
            "|  4| DDD|dept1|  1500|\n",
            "|  5| EEE|dept2|  8000|\n",
            "|  6| FFF|dept2|  7200|\n",
            "|  7| GGG|dept3|  7100|\n",
            "|  8| HHH|dept3|  3700|\n",
            "|  9| III|dept3|  4500|\n",
            "| 10| JJJ|dept5|  3400|\n",
            "+---+----+-----+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"select * from temp_table where salary >= 1500\").show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkv7tjPhMDKj"
      },
      "source": [
        "### Leyendo la tabla HIVE como DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "JZXQ0LzdMDKj",
        "outputId": "ac7d69ed-1269-42f6-c47e-b0db91c4880f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-51-29fe1597bb97>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    df = spark.table(\"DB_NAME\".\"TBL_NAME\")\u001b[0m\n\u001b[0m                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "# DB_NAME : Name of the the HIVE Database\n",
        "# TBL_NAME : Name of the HIVE Table\n",
        "\n",
        "\n",
        "df = spark.table(\"DB_NAME\".\"TBL_NAME\")\n",
        "#df = spark.table(\"DB_NAME.TBL_NAME\")\n",
        "#df = spark.table(\"DB_NAME\" + \".\" + \"TBL_NAME\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# analizando soluciones\n",
        "#db_name = \"DB_NAME\"\n",
        "#tbl_name = \"TBL_NAME\"\n",
        "#df = spark.table(f\"{db_name}.{tbl_name}\")\n"
      ],
      "metadata": {
        "id": "gfcU1wn6QeBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LEzgX-4MDKj"
      },
      "source": [
        "### Guardar DataFrame como tabla HIVE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "UND1dDfIMDKj",
        "outputId": "2346b0e9-a112-4973-b740-f1648e9d87f4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-ff3cf1a468b2>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DB_NAME.TBL_NAME\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m## También podemos seleccionar el argumento \"modo\" con overwrite\", \"append\", \"error\" etc.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DB_NAME.TBL_NAME\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.3-bin-hadoop3.2/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msaveAsTable\u001b[0;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m     def json(self, path, mode=None, compression=None, dateFormat=None, timestampFormat=None,\n",
            "\u001b[0;32m/content/spark-3.2.3-bin-hadoop3.2/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.3-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: Database 'db_name' not found"
          ]
        }
      ],
      "source": [
        "df.write.saveAsTable(\"DB_NAME.TBL_NAME\")\n",
        "\n",
        "## También podemos seleccionar el argumento \"modo\" con overwrite\", \"append\", \"error\" etc.\n",
        "df.write.saveAsTable(\"DB_NAME.TBL_NAME\", mode=\"overwrite\")\n",
        "\n",
        "# De forma predeterminada, la operación guardará el DataFrame como una tabla interna / administrada de HIVE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRqIyaAfMDKj"
      },
      "source": [
        "### Guardar el DataFrame como una tabla externa HIVE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "uaJxrgD1MDKj",
        "outputId": "a34fc9ec-3a36-4010-f304-1748787e826e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-54-c2ac387cc741>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    df.write.saveAsTable(\"DB_NAME.TBL_NAME\", path=<location_of_external_table>)\u001b[0m\n\u001b[0m                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "df.write.saveAsTable(\"DB_NAME.TBL_NAME\", path=<location_of_external_table>)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEZX2ZUCMDKj"
      },
      "source": [
        "### Crea un DataFrame a partir de un archivo CSV\n",
        "* Podemos crear un DataFrame usando un archivo CSV y podemos especificar varias opciones como un separador, encabezado, esquema, inferSchema y varias otras opciones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "riMqjFDLMDKj",
        "outputId": "2d810508-2e1f-45c4-9889-2cea2e4b93f7"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-25658412c8bd>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"path_to_csv_file\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"|\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.2.3-bin-hadoop3.2/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.3-bin-hadoop3.2/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.3-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: file:/content/path_to_csv_file"
          ]
        }
      ],
      "source": [
        " df = spark.read.csv(\"path_to_csv_file\", sep=\"|\", header=True, inferSchema=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZ-A4NuMMDKk"
      },
      "source": [
        "### Guardar un DataFrame como un archivo CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "AH4e94xCMDKk"
      },
      "outputs": [],
      "source": [
        "df.write.csv(\"path_to_CSV_File\", sep=\"|\", header=True, mode=\"overwrite\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54yTAD3CMDKk"
      },
      "source": [
        "### Crea un DataFrame a partir de una tabla relacional\n",
        "* Podemos leer los datos de bases de datos relacionales usando una URL JDBC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "LYK4Ej_rMDKk",
        "outputId": "6268f3c3-b848-45cf-f4a1-cc89894d70e1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-57-6b4b19622b83>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    .options(url=url, dbtable= <TBL_NAME>, user= <USER_NAME>, password = <PASSWORD>)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ],
      "source": [
        "# url : a JDBC URL of the form jdbc:subprotocol:subname\n",
        "# TBL_NAME : Name of the relational table.\n",
        "# USER_NAME : user name to connect to DataBase.\n",
        "# PASSWORD: password to connect to DataBase.\n",
        "\n",
        "\n",
        "relational_df = spark.read.format('jdbc')\n",
        "                        .options(url=url, dbtable= <TBL_NAME>, user= <USER_NAME>, password = <PASSWORD>)\n",
        "                        .load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_yaNezXMDKk"
      },
      "source": [
        "### Guardar el DataFrame como una tabla relacional\n",
        "* Podemos guardar el DataFrame como una tabla relacional usando una URL JDBC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "uSv4NlhtMDKk",
        "outputId": "f882e2d1-84d5-4182-d51c-beb7b5116929"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-58-370183d135a6>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    relational_df.write.format('jdbc')\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ],
      "source": [
        "# url : a JDBC URL of the form jdbc:subprotocol:subname\n",
        "# TBL_NAME : Name of the relational table.\n",
        "# USER_NAME : user name to connect to DataBase.\n",
        "# PASSWORD: password to connect to DataBase.\n",
        "\n",
        "\n",
        " relational_df.write.format('jdbc')\n",
        "                    .options(url=url, dbtable= <TBL_NAME>, user= <USER_NAME>, password = <PASSWORD>)\n",
        "                    .mode('overwrite')\n",
        "                    .save()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}